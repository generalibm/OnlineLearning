## 机器学习笔记02

### SVM(Support Vector Machine)——支持向量机
前苏联，Vapnik发明

#### 线型模型——线性可分训练集(Linear Seperable)

定理：有一条可分=>无数条可分，那么哪条最好(先验条件)
    - Vapnik：定义衡量每一条线的性能指标——平行移动，直到穿过某一个或者某几个位置，定义两条平行线的距离$D$为性能指标，则性能最好的是D最大的那条直线。
    - 参考书籍：《支持向量机导论》
    - 数学描述：
        - 间隔(Margin): $D$
        - 支持向量(Support Vector)：平行线穿过的向量，只与支持向量有关，故仅适用于小样本
    - 定义：
        1. 训练数据及标签 $(x_{1},y_{1}),  (x_{2}, y_{2}), ... , (x_{n}, y_{n})$——$x$是向量，$y$是标签(只能是+1和-1两类)
        2. 线性模型 $(w, b)$——$w$是向量且与$x$同维，b是一个常数
            1. 超平面(Hyperplane)方程——$\mathbf{w}^\mathrm{T}x + b = 0$
            2. 机器学习算法——求出所有$w$与b
        3. 一个训练集线性可分
            1. ${(x_{i}, y_{i})}, i = 1, 2, ... ,N$
            2. $${\exists}(w, b)$$，使得对于$${\forall} i = 1, 2, ... ,N$$
                1) 若$y_{i} = +1$， 则$\mathbf{w}^\mathrm{T}x + b >= 0$
                2) 若$y_{i} = -1$， 则$\mathbf{w}^\mathrm{T}x + b < 0$
                都成立
            3. 也即 $y_{i}$[$\mathbf{w}^\mathrm{T}$x + b] >= 0    (公式1) 成立
    - 支持向量集完成的工作——优化，距离最大的超平面怎样取到:
        0. 优化问题
            1. 最小化(Minimize)——$\frac{1}{2}\left \| w \right \| ^{2}$
            2. 限制条件(Subject to)——$y_{i}$[$\mathbf{w}^\mathrm{T}$x + b] >= 1
        1. 从**公式1**到**优化问题**的推导过程——注意到如下事实：
            1. **事实1**：$\mathbf{w}^\mathrm{T}$x + b = 0 与  $a \mathbf{w}^\mathrm{T}x + ab = 0$是同一个平面，其中 $ a \in R^{+}$；也即：若(w, b) 满足**公式1**， 则(aw, ab)也满足**公式1**
            2. **事实2**: 点$(x_{0}, y_{0})$到平面$w_{1}x + w_{2}y + b = 0$的距离 $D = \frac{w_{1}x_{0} + w_{2}y_{0} + b}{\sqrt{w_{1}^2+w_{2}^2}}$
        2. 即，向量$x_{0}$到超平面$\mathbf{w}^\mathrm{T}x + b = 0$的距离 $D = \frac{\mathbf{w}^\mathrm{T}x_{0} + b}{\left \| w \right \|}$
        3. 也即，我们可以用$a$去缩放$(w, b)-> (aw, ab)$，最终使得在支持向量$x_{0}$上，有$\left \| \mathbf{w}^\mathrm{T}x_{0} \right \| + b = 1$
        4. 因此，此优化问题也即**凸优化问题**或者**二次优化问题**
            1. 目标函数(Objective Function)是二次多项式
            2. 限制条件是一次多项式
            3. 故其解空间为 1)无解 或者 2) 唯一的极小值
            4. 计算机可以**试探**找出这个解：任意一个极值，随者**梯度减小**方向逐一试探，直到找到最小值

#### 非线性——非线性可分训练集(Non-Linear Seperable)




决策树



神经网络



深度学习算法——李飞飞